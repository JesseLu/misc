
\section{The objective-first design problem}
We now describe the remaining machinery used in the objective-first method,
    in addition to the field-solver and the structure-solver,
    as previously outlined.
Specifically, we introduce the idea of a design objective and a physics residual,
    and we reference the mathematical notion of convexity
    in order to motivate the need to divide the 
    objective-first problem into two separately convex sub-problems.

\subsection{Design objectives} \label{sec:desobj}
A design objective, $f(x)$, is simply defined as
    a function we wish to be minimal 
    for the design to be produced.

For instance, in the design of a device
    which must transmit efficiently into a particular mode,
    we could choose $f(x)$ to be the negative power flow into that mode.
Or, if the device was to be a low-loss resonator,
    we could choose $f(x)$ to be the amount of power leaking 
    out of the device.

In general, there are multiple choices of $f(x)$
    which can be used to describe the same objective.
For example, $f(x)$ for a transmissive device 
    may not only be the negative power transmitted into the desired output mode,
    but it could also be the amount of power lost to other modes,
    or even the error in the field values at the output port
    relative to the field values needed for perfect transmission.
These design objectives are equivalent in the sense that, if minimized, 
    all would produce structures with good performance.
At the same time, we must consider that the computational cost and complexity
    of using one $f(x)$ over another may indeed vary greatly.

\subsection{Convexity}
Before formulating the design problem,
    we would like to add a note regarding the complexity of various 
    optimization problems.

Specifically, we want to introduce the notion of \emph{convexity}\cite{boyd}
    and to note the difference between problems that
    are convex and those which are not.
The difference is simply this:
    convex problems have a single optimum point
    (only one local optimum, which is therefore the global optimum)
    which we can reliably find using existing numerical software,
    whereas non-convex problems typically have multiple optima 
    and are thus much more difficult to reliably solve.

That a convex problem can be reliably solved, in this case, 
    means that regardless of the starting guess,
    convex optimization software will 
    always arrive at the globally optimal solution
    and will be able to numerically prove global optimality as well.
Thus, the advantage in formulating a design problem 
    in terms of convex optimization problems
    is to eliminate both the need to circumvent local optima and 
    any notion of randomness.

On a practical level, there exist mature convex optimization software packages
    among which is CVX, a convex optimization package written for Matlab\cite{cvx}, 
    which we use for the examples in this chapter.


\subsection{Typical design formulation}
We now examine the typical, and most straightforward formulation
    of the design problem,
    in order to relate and contrast it to the objective-first formulation.
The design problem for a physical structure is typically formulated as
\BA \minimize{x,p} f(x) \label{eq:typform} \\
    \subto A(p)x - b(p) = 0, \notag \EA
    which states that we would like to vary $x$ and $p$ simultaneously
    in order to decrease $f(x)$
    while always satisfying physics (e.g. the electromagnetic wave equation).

Since solving \ER{typform} is quite difficult in the general sense
    (simultaneously varying $x$ and $p$ makes the problem non-convex),
    traditional approaches have relied on either brute-force parameter search,
    or a gradient-descent method utilizing first-order derivatives.
In the gradient-descent case, solving \ER{typform} results in the
    well-known adjoint optimization method\cite{miller}. 

\subsection{Objective-first design formulation}
In contrast with the typical formulation, the objective-first formulation 
    simply switches the roles of 
    the wave equation and the design objective with one another,
\BA \minimize{x,p} \| A(p) x - b(p) \|^2 \label{eq:ob1:1} \\
    \subto f(x) = f_\text{ideal}. \label{eq:ob1:2} \EA
Although such a switch may seem trivial,
    and even silly at first,
    we show that it fundamentally changes the nature of the design problem
    and actually gains us advantages in our efforts at finding a solution.

This first fundamental change, as seen from \ER{ob1:1},
    is that we allow for non-zero residual in the electromagnetic wave equation.
This literally means that we allow for \emph{non-physical} $x$ and $p$,
    since $A(p) x - b(p) \ne 0$ is permissible.
And since $A(p) x - b(p)$ can now be a non-zero entity, 
    we choose to call it the \emph{physics residual}.  
The second fundamental change
    is that we always force the device to exhibit ideal performance,
    as seen from \ER{ob1:2}.
This, of course, ties in very closely with \ER{ob1:1} since
    ideal performance is usually not obtainable unless one
    allows for some measure of error in the underlying physics
    (non-zero physics residual).
As such, our strategy will be to vary $x$ and $p$
    in order to decrease the physics residual \ER{ob1:1} to zero,
    while always maintaining ideal performance.

The primary advantage in the objective-first formulation is that,
    although the full problem is still non-convex,
    it allows us to form two convex sub-problems, as we outline below.
In contrast to an adjoint method approach,
    in doing we can still access information regarding
    second-order derivatives, which greatly speeds up finding a solution.
An additional advantage is that our insistence
    that ideal performance be always attained
    provides a mechanism
    which can potentially ``override'' local optima 
    in the optimization process.

To this end we have found that such a strategy
    actually allows us to design very unintuitive devices
    which exhibit very good performance,
    even when starting from completely non-functional initial guesses.
Furthermore, we have found this to be true
    even true when the physics residual is never brought to exactly zero.

% From a numerical standpoint, 
%     although the objective-first formulation is still non-convex 
%     in its original form,
%     the bi-linearity of the physics residual term allows us to 
%     naturally break the original problem into two sub-problems
%     which we outline below.

In practice, we add an additional constraint to the original formulation, \cite{opex1}
    which is to set hard-limits on the allowable values of $p$,
    namely $p_0 \le p \le p_1$.
This is actually a relaxation of the ideal constraint,
    which would be to allow $p$ to only have discrete values,
    $p \in p_0, p_1$,
    but such a constraint would be essentially force us to only
    be able to perform brute force trial-and-error.

Our objective-first formulation is thus,
\BA \minimize{x,p} \| A(p) x - b(p) \|^2 \notag \\
    \subto f(x) = f_\text{ideal} \label{eq:ob1} \\
        & p_0 \le p \le p_1, \notag \EA
    which is still non-convex, but can be broken down into 
    two convex sub-problems, 
    the motivation being that each of these will be 
    able to be easily and reliably solved.

\subsection{Field sub-problem}
The first of these is the field sub-problem, 
    which simply involves fixing $p$ and independently optimizing $x$,
\BA \minimize{x} \| A(p) x - b(p) \|^2 \label{eq:Fsub} \\
    \subto f(x) = f_\text{ideal}. \notag \EA
This problem is convex, and actually quadratic,
    which means that it can even be solved 
    using standard numerical tools, in the same way 
    as a simple least-squares problem.

The field sub-problem can be thought of as an update to $x$ (field)
    where we try to ``fit'' the electromagnetic fields to the structure ($p$).
Of course, if it were not for the hard-constraint on the design objective,
    the field sub-problem would be able to perfectly fit $x$ to $p$.
This, it turns out, would exactly be a simulation.

\subsection{Structure sub-problem}
The second sub-problem is formulated by fixing $x$ and
    independently optimizing $p$.
At the same time, we use the bi-linearity property
    of the physics residual from \ER{bilinear}
    to rewrite the problem in a way that makes
    its convexity explicit,
\BA \minimize{p} \| B(x) p - d(x) \|^2 \label{eq:Ssub} \\
    \subto p_0 \le p \le p_1. \notag \EA
The structure sub-problem is also convex, but not quadratic because of the 
    inequality constraints on $p$.
However, use of the CVX package still allows us to obtain the result
    quickly and reliably.

Note that in an analogous fashion to the field sub-problem,
    the structure sub-problem attempts to fit $p$ to $x$,
    and is prevented from perfectly doing so by its own constraint.

Because neither sub-problem is capable of completely reducing the physics residual
    to zero, they must be used in an iterative manner in order to
    gradually decrease the physics residual.
To this end, we employ the alternating directions optimization method.

\subsection{Alternating directions}
We use a simple alternating directions scheme 
    to piece together \ER{Fsub} and \ER{Ssub},
    which is to say that we simply
    alternately solve each and continue until we reach some stopping point,
    normally measured by how much the physics residual has decreased.
\BA \text{Loop:} & & \notag \\ 
& \minimize{x} \| A(p) x - b(p) \|^2 \notag \\
 &    \subto f(x) = f_\text{ideal}. \notag \\ 
    \\
& \minimize{p} \| B(x) p - d(x) \|^2 \notag \\
    & \subto p_0 \le p \le p_1. \notag \EA
The alternating directions scheme is extremely simple
    and does not require additional processing
    of $x$ or $p$ outside of the two sub-problems,
    nor does it require the use of auxiliary variables.

The advantage of such the alternating directions method
    is that the physics residual is guaranteed to
    monotonically decrease with every iteration,
    which is useful in that no safeguards
    are needed to guard against ``rogue'' steps
    in the optimization procedure.
Note that this robustness stems from the fact that,
    among other things,
    each sub-problem does not rely on previous values of 
    the variable which is being optimized,
    but only on the variable which is held constant.

The disadvantage of such a simple scheme is that 
    the convergence is quite slow,
    although we have found it to be sufficient in our cases.
Related methods, such as the Alternating Directions Method of Multipliers\cite{admm}, 
    exhibit far better convergence.
