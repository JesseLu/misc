\documentclass{article}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{hyperref}
\input defs.tex
\newcommand{\inv}[1]{\frac{1}{#1}}
\newcommand{\tC}{\tilde{C}}

\title{Nanophotonic Inverse Design}
\author{Jesse Lu}
\begin{document}
\maketitle
\tableofcontents

\section{Introduction}
Our goal is to produce
    a software package capable of designing virtually any nanophotonic device.
The software must produce designs
    \begin{itemize}
    \item with exceptional device performance,
    \item which are easily manufacturable, and
    \item with computational efficiency.
    \end{itemize}

Such a software package would be extremely useful 
    in designing the components needed to guide light on a computer chip
    such as couplers, filters, absorbers, and multiplexors.

However, the scale of the problem as well as 
    the difficulty of inverting the underlying wave equation
    have been major obstacles in producing successful design algorithms.


\subsection{Problem statement}
We formulate the design problem in the following way,
    \begin{subequations}\begin{align}
    \text{minimize} \quad   & f(x) + g(p) \\
    \text{subject to} \quad & r(x, p) = 0
    \end{align}\label{prob stat}\end{subequations}
    where
    \begin{itemize}
    \item $x$ is the field variable,
    \item $p$ is the structure variable,
    \item $f(x)$ is the performance objective,
    \item $g(p)$ is the manufacturability objective,
    \item $f(x) + g(p)$ is generally referred to as the design objective, and
    \item $r(x,p)$ is the physics residual for which we use 
            the time-harmonic electromagnetic wave equation,
            \begin{equation}
            r(x,p) = 
                (\nabla \times \epsilon^{-1} \nabla \times - \mu \omega^2) H
                - \text{sources}.
            \end{equation}
    \end{itemize}

\subsubsection{Multimode formulation}
We often desire a single device to perform multiple functions,
    in this case we can modify \eqref{prob stat} in the following way,
    \begin{subequations}\begin{align}
    \text{minimize} \quad   & \sum_i f_i(x_i) + g(p) \\
    \text{subject to} \quad & r_i(x_i, p) = 0.
    \end{align}\label{multi stat}\end{subequations}

\subsection{Key insights}\label{key insights}
Generic nonlinear optimization routines are usually unable
    to solve \eqref{prob stat},
    because there are an extremely large (millions) of variables, 
    and because $r(x,p)$ often results in ill-conditioned matrices.
For this reason, we need to take advantage of key features of the problem.
\begin{itemize}
\item $r(x,p)$ is separably affine (bi-affine) in $x$ and $p$,
    \begin{equation}
    r(x,p) = A(p)x - b(p) = B(x) p - d(x), \label{bi-affine}
    \end{equation}
        this allows us to form two simpler sub-problems.
\item Simulators which compute $A(p)^{-1} z$ are available,
        where $z$ is an arbitrary vector, 
        even for very large systems (millions of variables).
\item Solving $B(x) p - d(x) = 0$ is possible with generic software, 
        because manufacturing processes severely limit
        the degrees of freedom of $p$ 
        (decreasing $p$ to thousands of variables).
\end{itemize}
    


\section{Adjoint method}
The adjoint method is a steepest-descent method
    on the space $r(x,p) = 0$,
    and relies upon the following linear approximations
    of the design objective and the physics residual,
    \begin{subequations}\begin{align}
    (f(x + \Delta x) + g(p + \Delta p)) - (f(x) + g(p)) &\approx
        \nabla f^T \Delta x + \nabla g^T \Delta p \label{d lin} \\ 
    r(x + \Delta x, p + \Delta p) - r(x, p) &\approx
        A(p) \Delta x + B(x) \Delta p. \label{r lin}
    \end{align}\end{subequations}

Assuming a starting point already satisfying $r(x,p) = 0$,
    we note that \eqref{r lin} must equal zero,
    in order to keep the physics residual at zero.
This allows us to form the following relationship between
    $\Delta x$ and $\Delta p$,
    \begin{subequations}\begin{align}
    A(p) \Delta x + B(x) \Delta p &= A \Delta x + B \Delta p = 0 \\
    \Delta x &= - A^{-1} B \Delta p,
    \end{align}\end{subequations}
    which allows us to write \eqref{d lin} only in terms of $\Delta p$,
    \begin{equation}
    \nabla f^T \Delta x + \nabla g^T \Delta p =
        - (\nabla f^T A^{-1} B - \nabla g) \Delta p.
    \end{equation}
Thus, we see that the steepest-descent direction is
    \begin{equation}
    \Delta p = B^T A^{-T} \nabla f - \nabla g.
    \end{equation}

\subsection{Computational cost}
The adjoint method proceeds by iteratively
    \begin{enumerate}
    \item updating $p$ along $\Delta p$, and then \label{adj p step}
    \item updating $x$ by solving $r(x,p)$ for fixed $p$. \label{adj x step}
    \end{enumerate}

Computationally, step \ref{adj p step} requires an $A^{-T}$ solve,
    and step \ref{adj x step} requires an $A^{-1}$ solve.
The strength of the adjoint method resides in the fact that 
    each of these operations corresponds to a \emph{single simulation},
    and so the entire iteration costs only two simulations.

\subsection{Multi-mode formulation}
If one wishes to design a multi-functional device,
    the adjoint method can also be applied to \eqref{multi stat}.
In this case,
    \begin{equation}
    \Delta p = \sum_i B_i^T A_i^{-T} \nabla f_i - \nabla g,
    \end{equation}
    and each iterations costs $2N$ simulations,
    where $N$ is the number of modes considered.
Note that the $2N$ simulations can be spread over
    $N$ different computational nodes,
    so that the total running time of the optimization is   
    virtually identical to the single-mode case.
    
\section{Alternating directions method}
An alternative optimization method is to iteratively solve
    \begin{subequations}\begin{align}
    \mathop{\rm minimize}_x \quad   & f(x) + \frac{\rho}{2} \| r(x,p) \|^2 \\
    \mathop{\rm minimize}_p \quad   & g(p) + \frac{\rho}{2} \| r(x,p) \|^2,
    \end{align}\end{subequations}
    where the coefficient $\rho$ can be 
    gradually increased in order to drive $r(x,p) \to 0$.

The alternating directions method allows a non-physical starting point,
    and takes advantage of bi-affine $r(x,p)$ in the sense that 
    the subproblems can be re-written as
    \begin{subequations}\begin{align}
    \mathop{\rm minimize}_x \quad   & f(x) + 
        \frac{\rho}{2} \| A(p)x - b(p) \|^2 \\
    \mathop{\rm minimize}_p \quad   & g(p) + 
        \frac{\rho}{2} \| B(x)p - d(x) \|^2,
    \end{align}\end{subequations}
    since each of these subproblems may be readily 
    solved by generic optimization tools,
    especially if both $f(x)$ and $g(p)$ are convex.

\section{Alternating directions method of multipliers (ADMM)}
The inclusion of a dual variable ($y$) allows 
    us to form an \emph{augmented Lagrangian},
    \begin{equation}
    \mathcal{L}(x, p, y) = f(x) + g(p) + 
        \frac{\rho}{2} \| r(x,p) \|^2 + y^T r(x,p).
    \end{equation}
This leads to the 
    alternating directions method of multipliers (ADMM) \cite{ADMM},
    which proceeds in the following way,
    \begin{subequations}\begin{align}
    x &:= \argmin_x \mathcal{L}(x,p,y) \\
    p &:= \argmin_p \mathcal{L}(x,p,y) \\
    y &:= y + \rho r(x,p).
    \end{align}\end{subequations}
Similarily to alternating directions, 
    ADMM allows for an infeasible starting point, $r(x,p) \neq 0$.
However, ADMM allows the coefficient $\rho$ to remain fixed,
    and generally demonstrates faster convergence.

\subsection{Computational cost of ADMM}
We infer that the majority of computational resources 
    will be used in updating $x$ since
    the $y$-update is trivial, and
    the $p$-update involves far fewer variables 
    (see section \ref{key insights}).

In this vein, we examine the computational work, for various choices of $f(x)$,
    required in efficiently solving
    \begin{equation}
    \argmin_x \mathcal{L}(x,p,y) = 
        \argmin_x f(x) + \frac{\rho}{2}\|Ax - b\|^2 + y^T(Ax - b)
    \end{equation}
    via Newton's method; that is, updating $x$ along $\Delta x$ given by
    \begin{equation}
    \Delta x = (\nabla_{xx}^2 \mathcal{L})^{-1} \nabla_x \mathcal{L},
    \end{equation}
    where
    \begin{subequations}\begin{align}
    \nabla_x \mathcal{L} &= \nabla f(x) + \rho A^T (A x - b + \inv{\rho}y) \\
    \nabla_{xx}^2 \mathcal{L} &= \nabla^2 f(x) + \rho A^T A.
    \end{align}\end{subequations}

Furthermore, we limit our analysis to choices of $f(x)$ for which 
    $\mathcal{L}(x,p,y)$ is quadratic,
    since the optimum is simply $x + \Delta x$ and
    requires only one calculation of $\Delta x$.
Newton's method, of course, is very capable of minimizing nonlinear functions,
    and adapting our analysis to nonlinear choices of $f(x)$ 
    simply requires multiple steps ($\Delta x$) to be computed.

\subsubsection{Linear $f(x)$}
We investigate the choice $f(x) = c^T x$, 
    where $c \in \reals^{n \times 1}$
    and $x \in \reals^{n \times 1}$.
Here,
    \begin{subequations}\begin{align}
    \nabla f(x) &= c \\
    \nabla^2 f(x) &= 0
    \end{align}\end{subequations}
    which results in
    \begin{subequations}\begin{align}
    \nabla_x \mathcal{L} &= c + \rho A^T (A x - b + \inv{\rho} y) 
        = \rho A^T (Ax - b + \inv{\rho}(y + \tilde{c})) \\
    \nabla_{xx}^2 \mathcal{L} &= \rho A^T A,
    \end{align}\end{subequations}
    where $\tilde{c} = A^{-T} c$.
A single Newton step is
    \begin{equation}\begin{split}
    \Delta x &= (\nabla_{xx}^2 \mathcal{L})^{-1} \nabla_x \mathcal{L} \\
        &= (\rho^{-1} A^{-1} A^{-T}) 
            A^T (A x - b + \inv{\rho}(y + \tilde c)) \\
        &= A^{-1}(A x - b + \inv{\rho}(y + \tilde c)).
    \end{split}\end{equation}

From this analysis we see that 
    the computation cost of a linear performance objective, $f(x)$, 
    for the $x$-update is two simulations.
That is, in a very similar fashion to the adjoint method,
    an $A^{-T}$ solve is required to compute $\tilde{c}$,
    and an additional $A^{-1}$ solve is needed to compute $\Delta x$.

\subsubsection{Low-rank quadratic $f(x)$}
We investigate the case where $f(x) = \frac{1}{2}\|C_1^T x - d_1\|^2$,
    where $C_1 \in \reals^{n \times p_1}$
    and $d_1 \in \reals^{p_1 \times 1}$, for $p_1 \ll n$.
Here,
    \begin{subequations}\begin{align}
    \nabla f(x) &= C_1 (C_1^T x - d_1) \\
    \nabla^2 f(x) &= C_1 C_1^T
    \end{align}\end{subequations}
    which results in
    \begin{subequations}\begin{align}
    \nabla_x \mathcal{L} &= \rho A^T ((I + \inv{\rho}\tC_1\tC_1^T)Ax
        - b + \inv{\rho}(y - \tC_1 d)) \\
    \nabla_{xx}^2 \mathcal{L} &= \tC_1\tC_1^T + \rho A^T A,
    \end{align}\end{subequations}
    where $\tC_1 = A^{-T}C_1$.

We then apply the matrix inversion lemma (see appendix \ref{inv lemma})
    \begin{equation}\begin{split}
    (\nabla_{xx}^2 \mathcal{L})^{-1} &= (\tC_1\tC_1^T + \rho A^T A)^{-1} \\
        &= \inv{\rho} A^{-1} 
            (I - \tC_1(\rho I + \tC_1^T\tC_1)^{-1}\tC_1^T) A^{-T} \\
        &= \inv{\rho} A^{-1} M A^{-T}
    \end{split}\end{equation}
    where $M = I - \tC_1(\rho I + \tC_1^T\tC_1)^{-1}\tC_1^T$.
Note that computing $M$ is computationally inexpensive
    since the matrix which must be inverted, $\rho I + \tC_1^T\tC_1$,
    is $p_1 \times p_1$ ($p_1 \ll n$).

The Newton step is given by
    \begin{equation}
    \Delta x = A^{-1}M ((I + \inv{\rho}\tC_1\tC_1^T)Ax
        - b + \inv{\rho}(y - \tC_1 d_1)),
    \end{equation}
    and requires $p_1$ solutions of $A^{-T}$ to compute $\tC_1$, 
    and then 1 solution of $A^{-1}$ to compute $\Delta x$.


\subsubsection{$f(x)$ composed of linear equality constraints}
We investigate the situation where $f(x)$ involves
    satisfying the equality constraint $C_2^T x = d_2$,
    where $C_2 \in \reals^{n \times p_2}$
    and $d_2 \in \reals^{p_2 \times 1}$.
Once again, we assume that $p_2 \ll n$.

Newton's method for a linearly-constrained quadratic function is
    \begin{equation}
    \begin{bmatrix} \nabla_{xx}^2 \mathcal{L} & C_2 \\ C_2^T & 0 \end{bmatrix} 
    \begin{bmatrix} \Delta x \\ v \end{bmatrix} = 
    \begin{bmatrix} -\nabla_x\mathcal{L} \\ C_2 x - d_2 \end{bmatrix}.
    \end{equation}
We assume that 
    \begin{subequations}\begin{align}
    (\nabla_{xx}^2 \mathcal{L})^{-1} &= A^{-1} M A^{-T} \\
    \nabla_x \mathcal{L} &= A^T z
    \end{align}\end{subequations}
    and solve for $\Delta x$ 
    using block elimination (see appendix \ref{block elim}),
    \begin{subequations}\begin{align}
    v &= -(\tC_2^T M \tC_2)^{-1}(\tC_2^T Ax - d_2 + \tC_2^T Mz) \\
    \Delta x &= -A^{-1} M (z + \tC_2 v),
    \end{align}\end{subequations}
    where $\tC_2 = A^{-T} C_2$.
    If we condense the block elimination procedure to a single step,
    we obtain
    \begin{equation}
    \Delta x = -A^{-1} M (z - 
        \tC_2 (\tC_2^T M \tC_2)^{-1}(\tC_2^T Ax - d_2 + \tC_2^T Mz)).
    \end{equation}


Once again, $p_2 + 1$ simulations are required, 
    because computing $\tC_2$ requires $p_2$ solves of $A^{-T}$ and
    computing $\Delta x$ requires 1 solve of $A^{-1}$.

\subsubsection{Combined performance objective}
As can be seen from the previous examples,
    efficiently computing $\Delta x$ involves transforming $Ax \to z$,
    and then converting back, $A^{-1}z \to x$.
It should be no surprise then, 
    that we can also include all three performance objectives 
    in a single $f(x)$.

This results in computing
    \begin{equation}
    \Delta x = -A^{-1} M (z - 
        \tC_2 (\tC_2^T M \tC_2)^{-1}(\tC_2^T Ax - d_2 + \tC_2^T Mz)),
    \end{equation}
    where 
    \begin{subequations}\begin{align}
    M &= I - \tC_1(\rho I + \tC_1^T\tC_1)^{-1}\tC_1^T \\
    \begin{split}
    z &= A^{-T} \nabla_x \mathcal{L} \\
      &= \rho((I + \inv{\rho}\tC_1\tC_1^T)Ax
        - b + \inv{\rho}(y +\tilde{c} - \tC_1 d_1))
    \end{split} \\
    \tilde{c} &= A^{-1} c \\
    \tC_1 &= A^{-1} C_1 \\
    \tC_2 &= A^{-1} C_2. 
    \end{align}\end{subequations}

As expected, the first transformation costs $1 + p_1 + p_2$ solves of $A^{-T}$,
    and the second transformation costs 1 solve of $A^{-1}$.

\subsubsection{Concave, low-rank quadratic $f(x)$}
As a special case, we now consider the choice of 
    $f(x) = - \frac{1}{2}\|C_1^T x \|^2$,
    which is a quadratic, but concave, performance objective.
We consider the case where $f(x)$ is low-rank, that is,
    $C_1 \in \reals^{n \times p_1}$ where $p_1 \ll n$.

To deal with the concavity of $f(x)$ we would like to use
    a truncated Newton method,
    \begin{equation}
    \hat{\mathcal{L}}(x,p,y) = \mathcal{L}(x,p,y) + \frac{\eta}{2} \|x - x_0\|^2
    \end{equation}
    or even
    \begin{equation}
    \Delta x = -(\nabla_{xx}^2 \mathcal{L} + \eta I)^{-1}
        \nabla_x \mathcal{L},
    \end{equation}
    where the coefficient $\eta$ is increased in order to 
    ensure that the augmented Hessian is positive definite 
    (so that $(\nabla_x\mathcal{L})^T\Delta x > 0$).
However, this generally makes inverting the Hessian intractable.

A computationally efficient alternative would be to augment $\mathcal{L}(x,p,y)$
    in the following way,
    \begin{equation}
    \hat{\mathcal{L}}(x,p,y) = \mathcal{L}(x,p,y) + 
        \frac{\eta}{2} \|A(x - x_0)\|^2
    \end{equation}
    so that
    \begin{equation}
    (\nabla_{xx}^2 \hat{\mathcal{L}})^{-1} = 
        A^{-1} (\rho I + \eta I - \tC_1 \tC_1^T)^{-1} A^{-T}
    \end{equation}
    where $\tC_1 = A^{-T} C_1$ and $\nabla_{xx}^2 \mathcal{L} = \rho A^T A$.

For the case $p_1 = 1$, positive definiteness can be ensured by choosing
    $\eta = \| \tC_1 \|^2$.
Generally, for $p_1 > 1$, $\eta$ can be set to the extremal eigenvalue of
    $\tC_1 \tC_1^T$ in order to ensure positive definiteness.
Efficient computation of such an eigenvalue can be performed
    via a $QR$ factorization of $\tC_1$.

The computational cost of using a truncated Newton method is that 
    multiple steps are needed, and so $\Delta x$ must be computed many times.
Fortunately, $\tC = A^{-T} C$ needs only to be computed once,
    bringing the total number of solves to $n_{tn} + p_1$, 
    where $n_{tn}$ is the number of steps needed 
    for the truncated Newton method.

\subsubsection{Linearly constrained $f(x)$ with arbitrary phase}
We now investigate another special case of $f(x)$, 
    namely, when $f(x)$ implements the equality constraint 
    $C_2^T x = e^{i \phi}d_2$,
    where $C_2 \in \reals^{n \times p_2}$ and $p_2 \ll n$.
This situation corresponds to
    the addition of an arbitrary phase $e^{i\phi}$
    to a linear equality constraint.
    
We simply note that this case can be handled by computing
    \begin{equation}
    \Delta x = -A^{-1} M (z - 
        \tC_2 (\tC_2^T M \tC_2)^{-1}(\tC_2^T Ax - e^{i\phi}d_2 + \tC_2^T Mz)).
    \end{equation}
    for different choices of $\phi$.
The total computational cost is then simply $n_\phi + p_2$ solves,
    where $n_\phi$ is the number of values of $\phi$ which are tried.

\subsection{Multi-mode ADMM}
Since ADMM explicitly separates updating the field and structure variables,
    handling multiple fields is very naturally included in the formulation.
Namely, multiple fields ($x_i$) are updated simultaneously, in parallel,
    and then care ``connected'' by updating a single $p$ 
    which takes all the $x_i$ into account.

\begin{appendix}
\section{Constructing the relevant matrices and vectors}
\section{Solving the matrices}
\section{Linear algebra definitions}
\subsection{Gradient and Hessian calculations}
\subsection{Matrix inversion lemma}\label{inv lemma}
\begin{equation}
(A + UCV)^{-1} = A^{-1} - A^{-1}U (C^{-1} + V A^{-1} U)^{-1} V A^{-1}
\end{equation}
\subsection{Block elimination of a matrix}\label{block elim}
\begin{equation}
\begin{bmatrix} A_{11} & A_{12} \\ A_{21} & A_{22} \end{bmatrix} 
\begin{bmatrix} x_1 \\ x_2 \end{bmatrix} = 
\begin{bmatrix} b_1 \\ b_2 \end{bmatrix}
\end{equation}
\begin{subequations}\begin{align}
A_{11} x_1 + A_{12} x_2 &= b_1 \\
x_1 &= A_{11}^{-1} (b_1 - A_{12} x_2)
\end{align}\end{subequations}
\begin{subequations}\begin{align}
A_{21} x_1 + A_{22} x_2 &= b_2 \\
A_{21} A_{11}^{-1} (b_1 - A_{12} x_2) + A_{22} x_2 &= b_2 \\
(A_{22} - A_{21} A_{11}^{-1} A_{12}) x_2 &= b_2 - A_{21} A_{11}^{-1} b_1
\end{align}\end{subequations}
Typically, one computes $A_{11}^{-1} A_{12}$ and $A_{11}^{-1} b_1$, 
    and then solves for the rest of the system.
\end{appendix}
\begin{thebibliography}{99}
\bibitem{ADMM} Boyd group ADMM paper
\end{thebibliography}
\end{document}
